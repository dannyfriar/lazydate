{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from typing import List, Tuple, Dict, Optional, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "import babel\n",
    "import nlpaug.augmenter.char as nac\n",
    "from babel.dates import format_date, format_datetime\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    RepeatVector,\n",
    "    TimeDistributed,\n",
    "    Embedding,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "os.chdir(\"/Users/danny/Desktop/lazydate\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read wikitext-103\n",
    "with open('data/wiki.train.raw', 'r') as f:\n",
    "    wikitext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = sent_tokenize(wikitext[:10000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Irish rugby has become increasingly competitive at both the international and provincial levels since the sport went professional in 1994 .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(wiki_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_formats = [\"d\", \"dd\"]\n",
    "month_formats = [\"M\", \"MM\", \"MMM\", \"MMMM\", \"MMMM\", \"L\", \"LL\", \"LLL\", \"LLLL\", \"LLLL\"]\n",
    "year_formats = [\"yy\", \"yyyy\" ]\n",
    "\n",
    "second_formats = [\"s\", \"ss\"]\n",
    "minute_formats = [\"m\", \"mm\"]\n",
    "hour_formats = [\"h\", \"hh\", \"H\", \"HH\"]\n",
    "timezone_formats = [\"\", \"\", \"\", \"\", \"\", \"z\", \"zz\", \"zzz\", \"zzzz\"]\n",
    "time_separators = [\":\"]\n",
    "\n",
    "separator_frequency = {\n",
    "    \".\": 0.1, \n",
    "    \"/\": 0.15, \n",
    "    \"-\": 0.15, \n",
    "    \"''\": 0.1,\n",
    "    \" \": 0.5, \n",
    "}\n",
    "\n",
    "built_in_formats = [\"short\", \"medium\", \"long\", \"full\"]\n",
    "\n",
    "locales = babel.localedata.locale_identifiers()\n",
    "locales = [l for l in locales if \"en_\" in l]\n",
    "\n",
    "\n",
    "def random_date(n_years: int = 100) -> Tuple[datetime.datetime, Dict[str, int]]:\n",
    "    start_date = datetime.datetime(1900, 1, 1, 0, 0, 0)\n",
    "    gen_dict = {\n",
    "        \"days\": np.random.randint(0, n_years * 265),\n",
    "        \"hours\": np.random.randint(0, 24),\n",
    "        \"minutes\": np.random.randint(0, 60),\n",
    "        \"seconds\": np.random.randint(0, 60),\n",
    "    }\n",
    "    \n",
    "    date = start_date + datetime.timedelta(**gen_dict)\n",
    "    return date, gen_dict\n",
    "\n",
    "\n",
    "def random_format(date: datetime.datetime) -> Tuple[str, Dict[str, str]]:\n",
    "    possible_separators = list(separator_frequency.keys())\n",
    "    \n",
    "    if date.year >= datetime.datetime.now().year + 1:\n",
    "        year_format = np.random.choice(year_formats)\n",
    "    else:\n",
    "        year_format = \"yyyy\"\n",
    "    \n",
    "    append_time = np.random.rand() <= 0.5\n",
    "    gen_dict = {\n",
    "        \"day\": np.random.choice(day_formats),\n",
    "        \"month\": np.random.choice(month_formats),\n",
    "        \"year\": year_format,\n",
    "        \"separator\": np.random.choice(\n",
    "            possible_separators, p=list(separator_frequency.values())\n",
    "        ),\n",
    "        \"append_time\": append_time,\n",
    "    }\n",
    "    if append_time:\n",
    "        time_gen_dict = {\n",
    "            \"second\": np.random.choice(second_formats),\n",
    "            \"minute\": np.random.choice(minute_formats),\n",
    "            \"hour\": np.random.choice(hour_formats),\n",
    "            \"timezone\": np.random.choice(timezone_formats),\n",
    "            \"time_separator\": np.random.choice(time_separators)\n",
    "        }\n",
    "    else:\n",
    "        time_gen_dict = {k: \"\" for k in [\"second\", \"minute\", \"hours\", \"timezone\", \"time_separator\"]}\n",
    "    gen_dict.update(time_gen_dict)\n",
    "    \n",
    "    sep = gen_dict[\"separator\"]\n",
    "    if sep != \"''\" and gen_dict[\"year\"] == \"yy\":\n",
    "        if np.random.random() <= 0.5:\n",
    "            gen_dict[\"year\"] = \"''\" + gen_dict[\"year\"]\n",
    "            \n",
    "    format_date_str = f\"{gen_dict['day']}{sep}{gen_dict['month']}{sep}{gen_dict['year']}\"\n",
    "    format_time_str = \"\"\n",
    "    \n",
    "    if append_time:\n",
    "        sep = gen_dict[\"time_separator\"]\n",
    "        format_time_str = f\" {gen_dict['hour']}{sep}{gen_dict['minute']}\"\n",
    "        if np.random.random() <= 0.5:\n",
    "            format_time_str += f\"{sep}{gen_dict['second']}\"\n",
    "        if np.random.random() <= 0.5:\n",
    "            format_time_str += f\" a\"  # AM / PM\n",
    "        if np.random.random() <= 0.5:\n",
    "            format_time_str += f\" {gen_dict['timezone']}\"    \n",
    "            \n",
    "    format_str = format_date_str + format_time_str\n",
    "    gen_dict[\"format_str\"] = format_str\n",
    "    return format_str, gen_dict\n",
    "\n",
    "\n",
    "def get_random_wiki_sentence() -> str:\n",
    "    idx = np.random.randint(0, len(wiki_sentences))\n",
    "    return wiki_sentences[idx]\n",
    "\n",
    "\n",
    "def random_noise_dict(\n",
    "    date: datetime.datetime, format_dict: Dict[str, str]\n",
    ") -> Dict[str, str]:\n",
    "\n",
    "    append_day_suffix = format_dict[\"day\"] == \"dd\" and np.random.random() <= 0.5\n",
    "    place_in_sentence = np.random.random() <= 0.5\n",
    "    \n",
    "    gen_dict = {\n",
    "        \"locale\": np.random.choice(locales),\n",
    "        \"append_day_suffix\": append_day_suffix,\n",
    "        \"aug_char_action\": np.random.choice([\"insert\", \"substitute\"]),\n",
    "        \"place_in_sentence\": place_in_sentence,\n",
    "        \"sentence\": get_random_wiki_sentence() if place_in_sentence else \"\",\n",
    "    }\n",
    "    \n",
    "    day_suffix = \"\"\n",
    "    if append_day_suffix:\n",
    "        if date.day in [1, 21, 31]:\n",
    "            day_suffix = \"st\"\n",
    "        elif date.day in [2, 22]:\n",
    "            day_suffix = \"st\"\n",
    "        elif date.day in [3, 23]:\n",
    "            day_suffix = \"rd\"\n",
    "        else:\n",
    "            day_suffix = \"th\"\n",
    "    gen_dict[\"day_suffix\"] = day_suffix\n",
    "    \n",
    "    return gen_dict\n",
    "\n",
    "\n",
    "def put_datestr_in_sentence(datestr: str, sentence: str):\n",
    "    split_sentence = sentence.split(\" \")    \n",
    "    idx = np.random.randint(0, len(split_sentence))\n",
    "    split_sentence[idx] = datestr\n",
    "    return \" \".join(split_sentence)\n",
    "\n",
    "\n",
    "def apply_noise(datestr: str, format_dict: Dict[str, str], noise_dict: Dict[str, Any]) -> str:\n",
    "    out = datestr\n",
    "    sep = format_dict[\"separator\"]\n",
    "    sep = sep[0] if len(sep) > 1 else sep\n",
    "    \n",
    "    date_parts = datestr.split(sep)\n",
    "    \n",
    "    if noise_dict[\"append_day_suffix\"]:\n",
    "        date_parts[0] = date_parts[0] + noise_dict[\"day_suffix\"]\n",
    "        \n",
    "    # Add spelling mistake to month name\n",
    "    if len(format_dict[\"month\"]) > 2 and np.random.random() <= 0.3:\n",
    "        aug = nac.RandomCharAug(\n",
    "            action=noise_dict[\"aug_char_action\"],\n",
    "            aug_char_min=1, \n",
    "            aug_char_max=1,\n",
    "        )\n",
    "        date_parts[1] = aug.augment(date_parts[1])\n",
    "        \n",
    "    out = f\"{sep}\".join(date_parts)\n",
    "    \n",
    "    if noise_dict[\"place_in_sentence\"]:\n",
    "        out = put_datestr_in_sentence(out, noise_dict[\"sentence\"])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_date(\n",
    "    no_date_prob: float = 0.1\n",
    ") -> Tuple[str, datetime.datetime, Dict[str, Any]]:\n",
    "    date, date_gen_dict = random_date()\n",
    "    format_str, format_gen_dict = random_format(date)\n",
    "    noise_gen_dict = random_noise_dict(date, format_gen_dict)\n",
    "\n",
    "    datestr = format_datetime(\n",
    "        date, format=format_str, locale=noise_gen_dict[\"locale\"],\n",
    "    )\n",
    "    datestr = apply_noise(datestr, format_gen_dict, noise_gen_dict)\n",
    "    \n",
    "    gen_dict = date_gen_dict\n",
    "    gen_dict.update(format_gen_dict)\n",
    "    gen_dict.update(noise_gen_dict)\n",
    "    gen_dict[\"no_date\"] = False\n",
    "    \n",
    "    # Example with no date\n",
    "    if np.random.random() <= no_date_prob:\n",
    "        date = None\n",
    "        datestr = get_random_wiki_sentence()\n",
    "        gen_dict[\"no_date\"] = True\n",
    "    \n",
    "    return datestr, date, gen_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date string: 20th.11.1963\n",
      "\n",
      "Correct Date: 1963-11-20 01:59:20\n",
      "\n",
      "CPU times: user 1.46 ms, sys: 1.5 ms, total: 2.95 ms\n",
      "Wall time: 9.71 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datestr, date, gen_dict = generate_date()\n",
    "print(f\"Date string: {datestr}\\n\")\n",
    "print(f\"Correct Date: {date}\\n\")\n",
    "# print(gen_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19631120'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.strftime(format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "DIGITS = \"0123456789\"\n",
    "SYMBOLS = \"£&()[]+-/*;:@_\\\\\\\"'#\" + \"€$%!?,. \"\n",
    "VOCABULARY = LETTERS + DIGITS + SYMBOLS\n",
    "\n",
    "MAX_SEQUENCE_LEN = 150\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "class CharVectorizer:\n",
    "    def __init__(\n",
    "        self, vocabulary: str, max_sequence_len: int = MAX_SEQUENCE_LEN\n",
    "    ):\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.encoder: Dict[str, int] = {c: idx for idx, c in enumerate(list(vocabulary))}\n",
    "        self.encoder[UNK_TOKEN] = len(self.encoder)\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return sorted(list(self.encoder.keys()))\n",
    "\n",
    "    def transform(self, inputs: List[str]) -> np.ndarray:\n",
    "        outputs = [self._get_char_indices_for_word(s) for s in inputs]\n",
    "        outputs = np.array(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _get_char_indices_for_word(self, text: str) -> np.ndarray:\n",
    "        next_arr = np.zeros([self.max_sequence_len], dtype=np.int32)\n",
    "\n",
    "        for idx, token in enumerate(text):\n",
    "            if idx < self.max_sequence_len:  # truncate end of sentence if too long\n",
    "                if token in self.encoder:\n",
    "                    vocab_idx = self.encoder[token]\n",
    "                else:\n",
    "                    vocab_idx = self.encoder[UNK_TOKEN]\n",
    "                next_arr[idx] = vocab_idx\n",
    "        return next_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, batch_size=32, n_examples=50000):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_examples = n_examples\n",
    "        self.input_vectorizer = CharVectorizer(vocabulary=VOCABULARY)\n",
    "        self.output_vectorizer = CharVectorizer(vocabulary=DIGITS, max_sequence_len=8)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(math.ceil(self.n_examples / self.batch_size))\n",
    "    \n",
    "    @property\n",
    "    def input_sequence_len(self):\n",
    "        return self.input_vectorizer.max_sequence_len\n",
    "    \n",
    "    @property\n",
    "    def input_vocab_size(self):\n",
    "        return len(self.input_vectorizer.vocabulary)\n",
    "    \n",
    "    @property\n",
    "    def output_sequence_len(self):\n",
    "        return self.output_vectorizer.max_sequence_len\n",
    "    \n",
    "    @property\n",
    "    def output_vocab_size(self):\n",
    "        return len(self.output_vectorizer.vocabulary)\n",
    "        \n",
    "    def generate_string_batch(self) -> Tuple[List[str], List[str]]:\n",
    "        input_strings: List[str] = []\n",
    "        output_strings: List[str] = []\n",
    "            \n",
    "        for _ in range(self.batch_size):\n",
    "            datestr, date, gen_dict = generate_date()\n",
    "            if date:\n",
    "                output_datestr = date.strftime(format=\"%Y%m%d\")\n",
    "            else:\n",
    "                output_datestr = \"\".join([UNK_TOKEN] * self.output_vectorizer.max_sequence_len)\n",
    "            input_strings.append(datestr)\n",
    "            output_strings.append(output_datestr)\n",
    "            \n",
    "        return input_strings, output_strings\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        input_strings, output_strings = self.generate_string_batch()\n",
    "        inputs = {\"datestr\": self.input_vectorizer.transform(input_strings)}\n",
    "        outputs = {\"output_datestr\": self.output_vectorizer.transform(output_strings)}\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 1.21 ms, total: 13.9 ms\n",
      "Wall time: 14.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs, outputs = generator.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_acc(y_true, y_pred):\n",
    "    return K.mean(\n",
    "        K.all(\n",
    "            K.equal(\n",
    "                K.max(y_true, axis=-1),\n",
    "                K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
    "            ),\n",
    "            axis=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def lstm_encoder_decoder(\n",
    "    input_sequence_len: int,\n",
    "    input_vocab_size: int,\n",
    "    output_sequence_len: int,\n",
    "    output_vocab_size: int,\n",
    "    embedding_dim: int = 64,\n",
    "    lstm_hidden_dim: int = 64,\n",
    "    learning_rate: float = 1e-3,\n",
    "):\n",
    "    # Encoder\n",
    "    _input = Input(shape=(input_sequence_len,), dtype=\"int32\", name=\"datestr\")\n",
    "    embedding = Embedding(output_dim=embedding_dim, input_dim=input_vocab_size, mask_zero=True)(_input)\n",
    "    encoded = Bidirectional(LSTM(lstm_hidden_dim, return_sequences=False))(embedding)\n",
    "    \n",
    "    # Decoder\n",
    "    repeated = RepeatVector(output_sequence_len)(encoded)\n",
    "    decoded = LSTM(lstm_hidden_dim, return_sequences=True)(repeated)\n",
    "    _output = TimeDistributed(Dense(output_vocab_size, activation='softmax'))(decoded)\n",
    "    \n",
    "    model = Model(inputs=[_input], outputs={\"output_datestr\": _output})\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.3802 - accuracy: 0.5012WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.3800 - accuracy: 0.5013WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1563/1563 [==============================] - 251s 160ms/step - loss: 1.3800 - accuracy: 0.5013 - val_loss: 1.1692 - val_accuracy: 0.5664\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "  61/1563 [>.............................] - ETA: 3:25 - loss: 1.1508 - accuracy: 0.5702"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-7:\n",
      "Process Keras_worker_ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-9bdcbac4f0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lazydate-Xz_VEgM_-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=2, restore_best_weights=True\n",
    ")\n",
    "\n",
    "gen_train = DataGenerator()\n",
    "gen_val = DataGenerator()\n",
    "\n",
    "model = lstm_encoder_decoder(\n",
    "    input_sequence_len=gen_train.input_sequence_len,\n",
    "    input_vocab_size=gen_train.input_vocab_size,\n",
    "    output_sequence_len=gen_train.output_sequence_len,\n",
    "    output_vocab_size=gen_train.output_vocab_size,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    gen_train,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=gen_val,\n",
    "    max_queue_size=20,\n",
    "    workers=2,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazydate-Xz_VEgM_-py3.7",
   "language": "python",
   "name": "lazydate-xz_vegm_-py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
